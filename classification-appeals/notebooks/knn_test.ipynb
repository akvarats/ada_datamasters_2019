{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akvarats/dev/ai/ada_datamasters_2019/classification-appeals/.venv/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"../models/180/model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stop_words = [\n",
    "    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его',\n",
    "    'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от',\n",
    "    'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже',\n",
    "    'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом',\n",
    "    'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их',\n",
    "    'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда',\n",
    "    'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти',\n",
    "    'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при',\n",
    "    'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про',\n",
    "    'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой',\n",
    "    'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю',\n",
    "    'между',\n",
    "    'быть', 'мой', 'наш', 'ваш', 'их', 'его', 'её', 'их',\n",
    "    'этот', 'тот', 'где', 'который', 'либо', 'нибудь', 'нет', 'да'\n",
    "]\n",
    "\n",
    "punctuation_signs = [',', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morph_parse(w):\n",
    "    # TODO: вероятно не разбирает ещё какие-то части речи\n",
    "    grammars = {\n",
    "        'ADJF': 'ADJ',\n",
    "        'ADVB': 'ADV',\n",
    "        'INFN': 'VERB',\n",
    "    }\n",
    "    p = morph.parse(w)\n",
    "    try:\n",
    "        p = max(p, key=lambda x: (x.score, x.methods_stack[0][2]))  # взято из статьи на habr\n",
    "    except Exception:\n",
    "        p = p[0]\n",
    "    return p.normal_form, '_' + grammars.get(p.tag.POS, p.tag.POS) if p.tag.POS else None\n",
    "\n",
    "def get_clean_rusvectores_words(text):\n",
    "    words = []\n",
    "    text = text.replace('.', '. ')\n",
    "    sentences = nltk.sent_tokenize(text, language=\"russian\")\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence, language=\"russian\")\n",
    "        for t in tokens:\n",
    "            w, POS = morph_parse(t.strip(',.')) #\n",
    "            if not POS:  # TODO: просто пропускаем слова без части речи\n",
    "                continue\n",
    "            if w and w not in stop_words and w not in punctuation_signs:\n",
    "                words.append(w+POS)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def get_distance_matrix_for_corpus(corpus):\n",
    "    \"\"\" \"\"\"\n",
    "    X = [[i] for i in range(0, len(corpus))]\n",
    "    \n",
    "    distance = lambda x, y: model.wmdistance(corpus[int(x[0])][\"cleaned_rusvectores_words\"], corpus[int(y[0])][\"cleaned_rusvectores_words\"])\n",
    "    \n",
    "    return pairwise_distances(X, metric=distance, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "\n",
    "def read_corpus(file_path):\n",
    "    \"\"\" Читает корпус начального текста \"\"\"\n",
    "    result = []\n",
    "\n",
    "    with open(file_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        next(csv_reader)  # skip header\n",
    "        for row in csv_reader:\n",
    "            orig_text = row[1]\n",
    "            cleaned_rusvectores_words = get_clean_rusvectores_words(orig_text)\n",
    "            result.append(\n",
    "                dict(\n",
    "                    id=row[0],\n",
    "                    orig_text=orig_text,\n",
    "                    category=row[2],\n",
    "                    theme=row[3],\n",
    "                    executor=row[4],\n",
    "                    cleaned_rusvectores_words=cleaned_rusvectores_words\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n",
    "corpus = read_corpus(\"../input/NashDomRyazan-29-03-2019.csv\")[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_distance_matrix_for_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "categories = dict()  # name -> index\n",
    "for row in corpus:\n",
    "    if row[\"category\"] not in categories:\n",
    "        categories[row[\"category\"]] = len(categories)\n",
    "    targets.append(categories[row[\"category\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Дворовая территория': 0,\n",
       " 'Парковки': 1,\n",
       " 'Дороги': 2,\n",
       " 'Городская территория': 3,\n",
       " 'Остановки общественного транспорта': 4,\n",
       " 'Государственное управление и местное самоуправление': 5,\n",
       " 'Образование (школы, детские сады)': 6,\n",
       " 'Многоквартирные дома': 7,\n",
       " 'Бизнес': 8,\n",
       " 'Природные ресурсы и охрана окружающей среды': 9,\n",
       " 'ЖКХ': 10}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='precomputed',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knc = KNeighborsClassifier(n_neighbors=2, metric=\"precomputed\")\n",
    "\n",
    "knc.fit(d, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Разбитая (отсутствуют стёкла) ООТ \\\"Улица Пушкина\\\" в сторону Центра напротив дома ул. Ленинского Комсомола, 19. Просьба восстановить остекление.\"\n",
    "\n",
    "test_vectors = get_clean_rusvectores_words(text)\n",
    "\n",
    "test_distances = [model.wmdistance(test_vectors, c[\"cleaned_rusvectores_words\"]) for c in corpus]\n",
    "\n",
    "result = knc.predict_proba([test_distances])\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
